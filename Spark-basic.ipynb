{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HTML display of Spark `DataFrame`\n",
        "\n",
        "Load a DataFrame from a local `csv` file, and display it as readable HTML.\n",
        "\n",
        "\n",
        "## Imports \n",
        "\n",
        "(But first turn off overly verbose Spark logging.)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.log4j.{Level, Logger}\n",
        "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
        "\n",
        "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
        "\n",
        "import org.apache.spark.sql._\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a Spark context:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val spark = {\n",
        "  NotebookSparkSession.builder()\n",
        "    .master(\"local[*]\")\n",
        "    .getOrCreate()\n",
        "}\n",
        "\n",
        "def sc = spark.sparkContext"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define HTML display function\n",
        "\n",
        "This solution is lifted directly from a comment in this issue:\n",
        "\n",
        "https://github.com/almond-sh/almond/issues/180\n",
        "\n",
        "Define an implicit class to display DataFrames as HTML:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "implicit class RichDF(val ds:DataFrame) {\n",
        "    def showHTML(limit:Int = 20, truncate: Int = 20) = {\n",
        "        import xml.Utility.escape\n",
        "        val data = ds.take(limit)\n",
        "        val header = ds.schema.fieldNames.toSeq        \n",
        "        val rows: Seq[Seq[String]] = data.map { row =>\n",
        "          row.toSeq.map { cell =>\n",
        "            val str = cell match {\n",
        "              case null => \"null\"\n",
        "              case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
        "              case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
        "              case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
        "              case _ => cell.toString\n",
        "            }\n",
        "            if (truncate > 0 && str.length > truncate) {\n",
        "              // do not show ellipses for strings shorter than 4 characters.\n",
        "              if (truncate < 4) str.substring(0, truncate)\n",
        "              else str.substring(0, truncate - 3) + \"...\"\n",
        "            } else {\n",
        "              str\n",
        "            }\n",
        "          }: Seq[String]\n",
        "        }\n",
        "\n",
        "        publish.html(s\"\"\" <table>\n",
        "                <tr>\n",
        "                 ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
        "                </tr>\n",
        "                ${rows.map { row =>\n",
        "                  s\"<tr>${row.map{c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
        "                }.mkString}\n",
        "            </table>\n",
        "        \"\"\")        \n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and display a `DataFrame`\n",
        "\n",
        "Load data from a CSV file into a Spark `DataFrame`:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "val localCsvFile = \"train.csv\"\n",
        "\n",
        "val trainingSet =  spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(localCsvFile)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainingSet.showHTML()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "scala212"
    },
    "language_info": {
      "name": "scala",
      "version": "2.12.10",
      "mimetype": "text/x-scala",
      "file_extension": ".scala",
      "nbconvert_exporter": "script",
      "codemirror_mode": "text/x-scala"
    },
    "kernelspec": {
      "name": "scala212",
      "language": "scala",
      "display_name": "Scala212 (almond)"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}